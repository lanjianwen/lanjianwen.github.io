<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F11%2F05%2Ftest%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[蚁群算法求TSP问题]]></title>
    <url>%2F2019%2F11%2F05%2F%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95%E6%B1%82TSP%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[蚁群算法介绍 什么是蚁群算法蚁群算法就是模拟蚂蚁寻找食物的过程，它能够求出从原点出发，经过若干个给定的需求点，最终返回原点的最短路径。这也就是著名的旅行商问题。 蚁群算法原理1.蚂蚁在路径上释放信息素。2.碰到还没走过的路口，就随机挑选一条路走。同时，释放与路径长度有关的信息素。3.信息素浓度与路径长度成反比。后来的蚂蚁再次碰到该路口时，就选择信息素浓度较高路径。4.最优路径上的信息素浓度越来越大。5.最终蚁群找到最优寻食路径。 蚁群算法基本流程 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148%% 旅行商问题(TSP)优化%% 清空环境变量clear allclc%% 导入数据load citys_data.mat%% 计算城市间相互距离fprintf(&apos;Computing Distance Matrix... \n&apos;);n = size(citys,1);D = zeros(n,n);for i = 1:n for j = 1:n if i ~= j D(i,j) = sqrt(sum((citys(i,:) - citys(j,:)).^2)); else D(i,j) = 1e-4; end end end%% 初始化参数fprintf(&apos;Initializing Parameters... \n&apos;);m = 50; % 蚂蚁数量alpha = 1; % 信息素重要程度因子beta = 2; % 启发函数重要程度因子rho = 0.7; % 信息素挥发因子Q = 1; % 常系数Eta = 1./D; % 启发函数Tau = ones(n,n); % 信息素矩阵Table = zeros(m,n); % 路径记录表iter = 1; % 迭代次数初值iter_max = 150; % 最大迭代次数 Route_best = zeros(iter_max,n); % 各代最佳路径 Length_best = zeros(iter_max,1); % 各代最佳路径的长度 Length_ave = zeros(iter_max,1); % 各代路径的平均长度 %% 迭代寻找最佳路径figure;while iter &lt;= iter_max fprintf(&apos;迭代第%d次\n&apos;,iter); % 随机产生各个蚂蚁的起点城市 start = zeros(m,1); for i = 1:m temp = randperm(n); start(i) = temp(1); end Table(:,1) = start; % 构建解空间 citys_index = 1:n; % 逐个蚂蚁路径选择 for i = 1:m % 逐个城市路径选择 for j = 2:n tabu = Table(i,1:(j - 1)); % 已访问的城市集合(禁忌表) allow_index = ~ismember(citys_index,tabu); allow = citys_index(allow_index); % 待访问的城市集合 P = allow; % 计算城市间转移概率 for k = 1:length(allow) P(k) = Tau(tabu(end),allow(k))^alpha * Eta(tabu(end),allow(k))^beta; end P = P/sum(P); % 轮盘赌法选择下一个访问城市 Pc = cumsum(P); target_index = find(Pc &gt;= rand); target = allow(target_index(1)); Table(i,j) = target; end end % 计算各个蚂蚁的路径距离 Length = zeros(m,1); for i = 1:m Route = Table(i,:); for j = 1:(n - 1) Length(i) = Length(i) + D(Route(j),Route(j + 1)); end Length(i) = Length(i) + D(Route(n),Route(1)); end % 计算最短路径距离及平均距离 if iter == 1 [min_Length,min_index] = min(Length); Length_best(iter) = min_Length; Length_ave(iter) = mean(Length); Route_best(iter,:) = Table(min_index,:); else [min_Length,min_index] = min(Length); Length_best(iter) = min(Length_best(iter - 1),min_Length); Length_ave(iter) = mean(Length); if Length_best(iter) == min_Length Route_best(iter,:) = Table(min_index,:); else Route_best(iter,:) = Route_best((iter-1),:); end end % 更新信息素 Delta_Tau = zeros(n,n); % 逐个蚂蚁计算 for i = 1:m % 逐个城市计算 for j = 1:(n - 1) Delta_Tau(Table(i,j),Table(i,j+1)) = Delta_Tau(Table(i,j),Table(i,j+1)) + Q/Length(i); end Delta_Tau(Table(i,n),Table(i,1)) = Delta_Tau(Table(i,n),Table(i,1)) + Q/Length(i); end Tau = (1-rho) * Tau + Delta_Tau; % 迭代次数加1，清空路径记录表 % figure; %最佳路径的迭代变化过程 [Shortest_Length,index] = min(Length_best(1:iter)); Shortest_Route = Route_best(index,:); plot([citys(Shortest_Route,1);citys(Shortest_Route(1),1)],... [citys(Shortest_Route,2);citys(Shortest_Route(1),2)],&apos;o-&apos;); pause(0.3); iter = iter + 1; Table = zeros(m,n); % endend%% 结果显示[Shortest_Length,index] = min(Length_best);Shortest_Route = Route_best(index,:);disp([&apos;最短距离:&apos; num2str(Shortest_Length)]);disp([&apos;最短路径:&apos; num2str([Shortest_Route Shortest_Route(1)])]);%% 绘图figure(1)plot([citys(Shortest_Route,1);citys(Shortest_Route(1),1)],... [citys(Shortest_Route,2);citys(Shortest_Route(1),2)],&apos;o-&apos;);grid onfor i = 1:size(citys,1) text(citys(i,1),citys(i,2),[&apos; &apos; num2str(i)]);endtext(citys(Shortest_Route(1),1),citys(Shortest_Route(1),2),&apos; 起点&apos;);text(citys(Shortest_Route(end),1),citys(Shortest_Route(end),2),&apos; 终点&apos;);xlabel(&apos;城市位置横坐标&apos;)ylabel(&apos;城市位置纵坐标&apos;)title([&apos;蚁群算法优化路径(最短距离:&apos; num2str(Shortest_Length) &apos;)&apos;])figure(2)plot(1:iter_max,Length_best,&apos;b&apos;,1:iter_max,Length_ave,&apos;r:&apos;)legend(&apos;最短距离&apos;,&apos;平均距离&apos;)xlabel(&apos;迭代次数&apos;)ylabel(&apos;距离&apos;)title(&apos;各代最短距离与平均距离对比&apos;) 实验结果对比1.alpha = 1;beta = 5;rho = 0.5;2.alpha = 10;beta = 10;rho = 0.5;3.alpha = 1;beta = 3;rho = 0.7;4.alpha = 2;beta = 10;rho = 0.5; 实验分析1.第一组实验在28次迭代出现停滞现象2.第二组实验在10次迭代出现停滞现象3.第三组实验在20次迭代出现停滞现象4.第四组实验在30次迭代出现停滞现象可见alpha和beta过大会造成开始时算法收敛速度较快，在随后寻优过程中，迭代一定次数后，容易出现停滞现象。本次实验中，第三组是最优解。 实验总结1.信息素因素alpha反映蚂蚁在运动过程中所积累的信息量在知道蚁群搜索中的相对重要程度。2.启发函数因子beta，反映了启发式信息在知道蚁群搜索过程中的相对重要程度，其大小反映了蚁群巡游过程中小言行、确定性因素的作用强度。b过大是，蚂蚁在某个局部点上选择局优的可能性大。3.rho过小，在各路径上残留的信息素过多，导致无效路径被继续搜索，影响收敛速率。]]></content>
      <categories>
        <category>计算智能</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模糊聚类]]></title>
    <url>%2F2019%2F10%2F27%2F%E6%A8%A1%E7%B3%8A%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[模糊控制]]></title>
    <url>%2F2019%2F10%2F22%2F%E6%A8%A1%E7%B3%8A%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[模糊控制器设计实验 算法例子介绍一个人的最终成绩是靠智商和努力共同形成的。输入变量是智商和勤奋度，输出为一个人的成绩。智商，勤奋度，成绩都有三个模糊标记：高、中等、低 主要代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465%模糊控制器设计a=newfis(&apos;fuzzf&apos;); %创建新的模糊推理系统%输入1f1=1; a=addvar(a,&apos;input&apos;,&apos;智商&apos;,[-3*f1,3*f1]); %添加 e 的模糊语言变量a=addmf(a,&apos;input&apos;,1,&apos;高&apos;,&apos;zmf&apos;,[-3*f1,-1*f1]); %添加 e 的模糊语言变量的隶属度函数（z型）a=addmf(a,&apos;input&apos;,1,&apos;一般&apos;,&apos;trimf&apos;,[-3*f1,-2*f1,0]); %隶属度函数为三角形a=addmf(a,&apos;input&apos;,1,&apos;低&apos;,&apos;trimf&apos;,[-3*f1,-1*f1,1*f1]); %输入2f2=1;a=addvar(a,&apos;input&apos;,&apos;勤奋度&apos;,[-3*f2,3*f2]); %添加 ec 的模糊语言变量a=addmf(a,&apos;input&apos;,2,&apos;高&apos;,&apos;zmf&apos;,[-3*f2,-1*f2]); a=addmf(a,&apos;input&apos;,2,&apos;一般&apos;,&apos;trimf&apos;,[-3*f2,-2*f2,0]);a=addmf(a,&apos;input&apos;,2,&apos;低&apos;,&apos;trimf&apos;,[-3*f2,-1*f2,1*f2]);%输出f3=1.5;a=addvar(a,&apos;output&apos;,&apos;成绩&apos;,[-3*f3,3*f3]); %添加 u 的模糊语言变量a=addmf(a,&apos;output&apos;,1,&apos;高&apos;,&apos;zmf&apos;,[-3*f3,-1*f3]); a=addmf(a,&apos;output&apos;,1,&apos;中等&apos;,&apos;trimf&apos;,[-3*f3,-2*f3,0]);a=addmf(a,&apos;output&apos;,1,&apos;低&apos;,&apos;trimf&apos;,[-3*f3,-1*f3,1*f3]);%规则库rulelist=[1 1 1 1 1; %编辑模糊规则，后俩个数分别是规则权重和AND OR选项 1 2 2 1 1; 1 3 3 1 1; 2 1 1 1 1; 2 2 2 1 1; 2 3 3 1 1; 3 1 1 1 1; 3 2 2 1 1; 3 3 3 1 1;]; a=addrule(a,rulelist); %添加模糊规则函数showrule(a) %显示模糊规则函数a1=setfis(a,&apos;DefuzzMethod&apos;,&apos;centroid&apos;); %设置解模糊方法writefis(a1,&apos;fuzzf&apos;); %保存模糊系统a2=readfis(&apos;fuzzf&apos;); %从磁盘读出保存的模糊系统disp(&apos;fuzzy Controller table:e=[-3,+3],ec=[-3,+3]&apos;);%显示矩阵和数组内容%推理Ulist=zeros(3,3); %全零矩阵for i=1:7 for j=1:3 e(i)=-4+i; ec(j)=-4+j; Ulist(i,j)=evalfis([e(i),ec(j)],a2); %完成模糊推理计算 end end% Ulist=ceil(Ulist) %朝正无穷方向取整 Ulist %朝正无穷方向取整 %画出模糊系统figure(1); plotfis(a2); figure(2);plotmf(a,&apos;input&apos;,1);figure(3);plotmf(a,&apos;input&apos;,2);figure(4);plotmf(a,&apos;output&apos;,1); 运行结果 结论对一个人的智商和勤奋度共同促进下的成绩情况进行模糊控制两个输入，智商，勤奋度，分别三个模糊标记，共九种模糊规则]]></content>
      <categories>
        <category>计算智能</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[BP神经网络]]></title>
    <url>%2F2019%2F09%2F28%2FBP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[前言由于传统的感知器和线性神经网络有自身无法克服的缺陷，它们都不能解决线性不可分问题，因此在实际应用过程中受到了限制。而BP网络却拥有良好的繁泛化能力、容错能力以及非线性映射能力。因此成为应用最为广泛的一种神经网络。 基于BP算法的多层前馈型网络模型的拓扑结构图 BP算法的基本思想学习过程 第一阶段:第一阶段是信号的正向传播过程；输入信息通过输入层、隐层逐层处理并计算每个单元的实际输出值举个例子:比如你有100万钱，分别投资三个公司。W为投资某个公司的钱占所有钱的百分比，V为每个公司的利润率,收益即为Y=100W1V1+100W2V2+100+W3*V3 第二阶段:第二阶段是误差的反向传递过程；若在输入层未能得到期望的输出值，则逐层递归的计算实际输出和期望输出的差值（即误差），以便根据此差值调节权值。这种过程不断迭代，最后使得信号误差达到允许或规定的范围之内举个例子:有一个游戏叫做”数字炸弹”，规则:一般十个人以上玩，主持人出数字，其他人猜。主持人写下在1-100之间随便1个数字，不能让猜得人知道。其他的人就可以开始猜。如：56。每个人开始猜猜数字，如：A说:30 主持人说：30到100 ，B在从30到100中猜数字说：60，主持人在说：30到60，………… Z说：56，游戏结束主持人反馈的信息就是反向传播代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132% BP网络% BP神经网络的构建net=newff([-1 2;0 5],[3,1],&#123;&apos;tansig&apos;,&apos;purelin&apos;&#125;,&apos;traingd&apos;)net.IW&#123;1&#125;net.b&#123;1&#125;p=[1;2];a=sim(net,p)net=init(net);net.IW&#123;1&#125;net.b&#123;1&#125;a=sim(net,p)%net.IW&#123;1&#125;*p+net.b&#123;1&#125;p2=net.IW&#123;1&#125;*p+net.b&#123;1&#125;a2=sign(p2)a3=tansig(a2)a4=purelin(a3)net.b&#123;2&#125;net.b&#123;1&#125;net.IW&#123;1&#125;net.IW&#123;2&#125;0.7616+net.b&#123;2&#125;a-net.b&#123;2&#125;(a-net.b&#123;2&#125;)/ 0.7616help purelinp1=[0;0];a5=sim(net,p1)net.b&#123;2&#125;% BP网络% BP神经网络的构建net=newff([-1 2;0 5],[3,1],&#123;&apos;tansig&apos;,&apos;purelin&apos;&#125;,&apos;traingd&apos;)net.IW&#123;1&#125;net.b&#123;1&#125;%p=[1;];p=[1;2];a=sim(net,p)net=init(net);net.IW&#123;1&#125;net.b&#123;1&#125;a=sim(net,p)net.IW&#123;1&#125;*p+net.b&#123;1&#125;p2=net.IW&#123;1&#125;*p+net.b&#123;1&#125;a2=sign(p2)a3=tansig(a2)a4=purelin(a3)net.b&#123;2&#125;net.b&#123;1&#125;P=[1.2;3;0.5;1.6]W=[0.3 0.6 0.1 0.8]net1=newp([0 2;0 2;0 2;0 2],1,&apos;purelin&apos;);net2=newp([0 2;0 2;0 2;0 2],1,&apos;logsig&apos;);net3=newp([0 2;0 2;0 2;0 2],1,&apos;tansig&apos;);net4=newp([0 2;0 2;0 2;0 2],1,&apos;hardlim&apos;);net1.IW&#123;1&#125;net2.IW&#123;1&#125;net3.IW&#123;1&#125;net4.IW&#123;1&#125;net1.b&#123;1&#125;net2.b&#123;1&#125;net3.b&#123;1&#125;net4.b&#123;1&#125;net1.IW&#123;1&#125;=W;net2.IW&#123;1&#125;=W;net3.IW&#123;1&#125;=W;net4.IW&#123;1&#125;=W;a1=sim(net1,P)a2=sim(net2,P)a3=sim(net3,P)a4=sim(net4,P)init(net1);net1.b&#123;1&#125;help tansig% 训练p=[-0.1 0.5]t=[-0.3 0.4]w_range=-2:0.4:2;b_range=-2:0.4:2;ES=errsurf(p,t,w_range,b_range,&apos;logsig&apos;);%单输入神经元的误差曲面plotes(w_range,b_range,ES)%绘制单输入神经元的误差曲面pause(0.5);hold off;net=newp([-2,2],1,&apos;logsig&apos;);net.trainparam.epochs=100;net.trainparam.goal=0.001;figure(2);[net,tr]=train(net,p,t);title(&apos;动态逼近&apos;)wight=net.iw&#123;1&#125;bias=net.bpause;close;% 练p=[-0.2 0.2 0.3 0.4]t=[-0.9 -0.2 1.2 2.0]h1=figure(1);net=newff([-2,2],[5,1],&#123;&apos;tansig&apos;,&apos;purelin&apos;&#125;,&apos;trainlm&apos;);net.trainparam.epochs=100;net.trainparam.goal=0.0001;net=train(net,p,t);a1=sim(net,p)pause;h2=figure(2);plot(p,t,&apos;*&apos;);title(&apos;样本&apos;)title(&apos;样本&apos;);xlabel(&apos;Input&apos;);ylabel(&apos;Output&apos;);pause;hold on;ptest1=[0.2 0.1]ptest2=[0.2 0.1 0.9]a1=sim(net,ptest1);a2=sim(net,ptest2);net.iw&#123;1&#125;net.iw&#123;2&#125;net.b&#123;1&#125;net.b&#123;2&#125; 结果截图]]></content>
      <categories>
        <category>计算智能</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[github+hexo搭建博客]]></title>
    <url>%2F2019%2F09%2F20%2Fgithub-hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[利用Github+Hexo搭建个人博客 准备工作 申请github账号github官网 下载并安装 Node.js下载与安装教程 下载并安装 Git下载与安装教程 安装Hexo和本地部署博客4.1. 新建文件夹Hexo(以后代码都放这)4.2. 在 Hexo 目录下 鼠标右键选择Git Bash Here4.3. 输入命令npm install -g hexo-cli4.4. 查看是否安装成功，输入hexo -v 安装成功4.5. 输入hexo init回车-&gt;输入npm install之后会生成一些文件夹4.6. 输入hexo s-&gt;打开浏览器输入http://localhost:4000/即可看到本地博客部署 github搭建github博客 登陆自己的github账号 新建一个仓库 ：点击在右上角+-&gt;New repository-&gt;在Repository name 输入仓库名字 xxx.github.io以后仓库名就是你的博客地址。 配置SSH Key ：一般在你的C盘用户目录下-&gt;找到隐藏文件.ssh文件夹-&gt;有私钥id_rsa 公钥id_rsa.pub-&gt;复制公钥(如果没有.ssh文件夹，打开Git Bush输入 ssh-keygen -t rsa -C &quot;你的邮箱地址&quot;)在你的github选择settings进入到 在Git Bush输入ssh -T git@github.com查看是否配置成功 将本地博客部署到github 安装github插件依赖：在Hexo文件目录下打开Git Bash输入npm install hexo-deployer-git --save 修改站点文件(Hexo文件夹下_config.yml) 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:&lt;github账号名称&gt;/&lt;github账号名称&gt;.github.io.git branch: master 输入hexo g &amp;&amp; hexo d-&gt;在浏览器输入http://xxx.github.io即可看到本地博客部署到github上s]]></content>
  </entry>
  <entry>
    <title><![CDATA[K-Means]]></title>
    <url>%2F2019%2F09%2F17%2FK-Means%2F</url>
    <content type="text"><![CDATA[K-means算法介绍 从D中随机取k个元素，作为k个簇的各自的中心。 分别计算剩下的元素到k个簇中心的相异度，将这些元素分别划归到相异度最低的簇。 根据聚类结果，重新计算k个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均数。 将D中全部元素按照新的中心重新聚类。 重复第4步，直到聚类结果不再变化。 将结果输出。JAVA代码实现分装Point类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package Kmeans;public class Point &#123; private float[] localArray; private int id; private int clusterId; // 标识属于哪个类中心。 private float dist; // 标识和所属类中心的距离。 public Point(int id, float[] localArray) &#123; this.id = id; this.localArray = localArray; &#125; public Point(float[] localArray) &#123; this.id = -1; //表示不属于任意一个类 this.localArray = localArray; &#125; public float[] getlocalArray() &#123; return localArray; &#125; public int getId() &#123; return id; &#125; public void setClusterId(int clusterId) &#123; this.clusterId = clusterId; &#125; public int getClusterid() &#123; return clusterId; &#125; public float getDist() &#123; return dist; &#125; public void setDist(float dist) &#123; this.dist = dist; &#125; @Override public String toString() &#123; String result = "Point_id=" + id + " ["; for (int i = 0; i &lt; localArray.length; i++) &#123; result += localArray[i] + " "; &#125; return result.trim()+"] clusterId: "+clusterId+" dist: "+dist; &#125; @Override public boolean equals(Object obj) &#123; if (obj == null || getClass() != obj.getClass()) return false; Point point = (Point) obj; if (point.localArray.length != localArray.length) return false; for (int i = 0; i &lt; localArray.length; i++) &#123; if (Float.compare(point.localArray[i], localArray[i]) != 0) &#123; return false; &#125; &#125; return true; &#125; @Override public int hashCode() &#123; float x = localArray[0]; float y = localArray[localArray.length - 1]; long temp = x != +0.0d ? Double.doubleToLongBits(x) : 0L; int result = (int) (temp ^ (temp &gt;&gt;&gt; 32)); temp = y != +0.0d ? Double.doubleToLongBits(y) : 0L; result = 31 * result + (int) (temp ^ (temp &gt;&gt;&gt; 32)); return result; &#125;&#125; 封装Cluster123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package Kmeans;import java.util.ArrayList;import java.util.List; public class Cluster &#123; private int id;// 标识 private Point center;// 中心 private List&lt;Point&gt; members = new ArrayList&lt;Point&gt;();// 成员 public Cluster(int id, Point center) &#123; this.id = id; this.center = center; &#125; public Cluster(int id, Point center, List&lt;Point&gt; members) &#123; this.id = id; this.center = center; this.members = members; &#125; public void addPoint(Point newPoint) &#123; if (!members.contains(newPoint))&#123; members.add(newPoint); &#125;else&#123; System.out.println("样本数据点 &#123;"+newPoint.toString()+"&#125; 已经存在！"); &#125; &#125; public int getId() &#123; return id; &#125; public Point getCenter() &#123; return center; &#125; public void setCenter(Point center) &#123; this.center = center; &#125; public List&lt;Point&gt; getMembers() &#123; return members; &#125; @Override public String toString() &#123; String toString = "Cluster \n" + "Cluster_id=" + this.id + ", center:&#123;" + this.center.toString()+"&#125;"; for (Point point : members) &#123; toString+="\n"+point.toString(); &#125; return toString+"\n"; &#125;&#125; 求两个点的欧式距离12345678910111213141516171819202122package Kmeans;public class DistanceCompute &#123; /** * 求欧式距离 */ public double getEuclideanDis(Point p1, Point p2) &#123; double count_dis = 0; float[] p1_local_array = p1.getlocalArray(); float[] p2_local_array = p2.getlocalArray(); if (p1_local_array.length != p2_local_array.length) &#123; throw new IllegalArgumentException("length of array must be equal!"); &#125; for (int i = 0; i &lt; p1_local_array.length; i++) &#123; count_dis += Math.pow(p1_local_array[i] - p2_local_array[i], 2); &#125; return Math.sqrt(count_dis); &#125;&#125; 核心运行类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package Kmeans;import java.util.ArrayList;import java.util.HashSet;import java.util.List;import java.util.Random;import java.util.Set; public class KMeansRun &#123; private int kNum; //簇的个数 private int iterNum = 10; //迭代次数 private int iterMaxTimes = 100000; //单次迭代最大运行次数 private int iterRunTimes = 0; //单次迭代实际运行次数 private float disDiff = (float) 0.01; //单次迭代终止条件，两次运行中类中心的距离差 private List&lt;float[]&gt; original_data =null; //用于存放，原始数据集 private static List&lt;Point&gt; pointList = null; //用于存放，原始数据集所构建的点集 private DistanceCompute disC = new DistanceCompute(); private int len = 0; //用于记录每个数据点的维度 public KMeansRun(int k, List&lt;float[]&gt; original_data) &#123; this.kNum = k; this.original_data = original_data; this.len = original_data.get(0).length; //检查规范 check(); //初始化点集。 init(); &#125; /** * 检查规范 */ private void check() &#123; if (kNum == 0)&#123; throw new IllegalArgumentException("k must be the number &gt; 0"); &#125; if (original_data == null)&#123; throw new IllegalArgumentException("program can't get real data"); &#125; &#125; /** * 初始化数据集，把数组转化为Point类型。 */ private void init() &#123; pointList = new ArrayList&lt;Point&gt;(); for (int i = 0, j = original_data.size(); i &lt; j; i++)&#123; pointList.add(new Point(i, original_data.get(i))); &#125; &#125; /** * 随机选取中心点，构建成中心类。 */ private Set&lt;Cluster&gt; chooseCenterCluster() &#123; Set&lt;Cluster&gt; clusterSet = new HashSet&lt;Cluster&gt;(); Random random = new Random(); for (int id = 0; id &lt; kNum; ) &#123; Point point = pointList.get(random.nextInt(pointList.size())); // 用于标记是否已经选择过该数据。 boolean flag =true; for (Cluster cluster : clusterSet) &#123; if (cluster.getCenter().equals(point)) &#123; flag = false; &#125; &#125; // 如果随机选取的点没有被选中过，则生成一个cluster if (flag) &#123; Cluster cluster =new Cluster(id, point); clusterSet.add(cluster); id++; &#125; &#125; return clusterSet; &#125; /** * 为每个点分配一个类！ */ public void cluster(Set&lt;Cluster&gt; clusterSet)&#123; // 计算每个点到K个中心的距离，并且为每个点标记类别号 for (Point point : pointList) &#123; float min_dis = Integer.MAX_VALUE; for (Cluster cluster : clusterSet) &#123; float tmp_dis = (float) Math.min(disC.getEuclideanDis(point, cluster.getCenter()), min_dis); if (tmp_dis != min_dis) &#123; min_dis = tmp_dis; point.setClusterId(cluster.getId()); point.setDist(min_dis); &#125; &#125; &#125; // 新清除原来所有的类中成员。把所有的点，分别加入每个类别 for (Cluster cluster : clusterSet) &#123; cluster.getMembers().clear(); for (Point point : pointList) &#123; if (point.getClusterid()==cluster.getId()) &#123; cluster.addPoint(point); &#125; &#125; &#125; &#125; /** * 计算每个类的中心位置！ */ public boolean calculateCenter(Set&lt;Cluster&gt; clusterSet) &#123; boolean ifNeedIter = false; for (Cluster cluster : clusterSet) &#123; List&lt;Point&gt; point_list = cluster.getMembers(); float[] sumAll =new float[len]; // 所有点，对应各个维度进行求和 for (int i = 0; i &lt; len; i++) &#123; for (int j = 0; j &lt; point_list.size(); j++) &#123; sumAll[i] += point_list.get(j).getlocalArray()[i]; &#125; &#125; // 计算平均值 for (int i = 0; i &lt; sumAll.length; i++) &#123; sumAll[i] = (float) sumAll[i]/point_list.size(); &#125; // 计算两个新、旧中心的距离，如果任意一个类中心移动的距离大于dis_diff则继续迭代。 if(disC.getEuclideanDis(cluster.getCenter(), new Point(sumAll)) &gt; disDiff)&#123; ifNeedIter = true; &#125; // 设置新的类中心位置 cluster.setCenter(new Point(sumAll)); &#125; return ifNeedIter; &#125; /** * 运行 k-means */ public Set&lt;Cluster&gt; run() &#123; Set&lt;Cluster&gt; clusterSet= chooseCenterCluster(); boolean ifNeedIter = true; while (ifNeedIter) &#123; cluster(clusterSet); ifNeedIter = calculateCenter(clusterSet); iterRunTimes ++ ; &#125; return clusterSet; &#125; /** * 返回实际运行次数 */ public int getIterTimes() &#123; return iterRunTimes; &#125;&#125; 测试类123456789101112131415161718192021222324252627282930313233package Kmeans;import java.util.ArrayList;import java.util.Set; public class Main &#123; public static void main(String[] args) &#123; ArrayList&lt;float[]&gt; dataSet = new ArrayList&lt;float[]&gt;(); dataSet.add(new float[] &#123; 1, 2, 3 &#125;); dataSet.add(new float[] &#123; 3, 3, 3 &#125;); dataSet.add(new float[] &#123; 3, 4, 4&#125;); dataSet.add(new float[] &#123; 5, 6, 5&#125;); dataSet.add(new float[] &#123; 8, 9, 6&#125;); dataSet.add(new float[] &#123; 4, 5, 4&#125;); dataSet.add(new float[] &#123; 6, 4, 2&#125;); dataSet.add(new float[] &#123; 3, 9, 7&#125;); dataSet.add(new float[] &#123; 5, 9, 8&#125;); dataSet.add(new float[] &#123; 4, 2, 10&#125;); dataSet.add(new float[] &#123; 1, 9, 12&#125;); dataSet.add(new float[] &#123; 7, 8, 112&#125;); dataSet.add(new float[] &#123; 7, 8, 4&#125;); KMeansRun kRun =new KMeansRun(3, dataSet); Set&lt;Cluster&gt; clusterSet = kRun.run(); System.out.println("单次迭代运行次数："+kRun.getIterTimes()); for (Cluster cluster : clusterSet) &#123; System.out.println(cluster); &#125; &#125;&#125; 结果截图]]></content>
      <categories>
        <category>计算智能</category>
      </categories>
  </entry>
</search>
